{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyNAvL6Snz6biV0MgJ0YtQ77",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jquach410/hinf6210/blob/main/copy%20Assignment_4_B00680884.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Assignment 4: Classification – part 2"
      ],
      "metadata": {
        "id": "nGOoScE3TB5L"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task 1. Prepare the data\n",
        "\n",
        "• Include all the required code from assignments 1 and 2 and apply any changes that you think are necessary.\n",
        "\n",
        "• Explain what changes you made to the code from assignment 1, and what was the purpose of these changes, if any."
      ],
      "metadata": {
        "id": "A19hpwAiTJFJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the Drive helper and mount\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/content/drive')\n",
        "path = \"/content/drive/My Drive/Colab Notebooks/HINF6210/data/Dataset_Jack.csv\"\n",
        "\n",
        "np.random.seed(0) # set seed now since we are randomizing with splitting\n",
        "\n",
        "data = pd.read_csv(path, na_values='?') # specify that ? are missing  \n",
        "\n",
        "# clean names\n",
        "data.columns=data.columns.str.lower().str.replace(' ','_')\n",
        "\n",
        "data = data.replace({'t': True, 'f': False, 'M': True, 'F': False}) \n",
        "# turn strings into a boolean\n",
        "# male = T, female = F, doing this to maintain any missing values \n",
        "\n",
        "# rename because we turned sex into a boolean\n",
        "data = data.rename(columns={\"sex\": \"sex_male\"}) # more informative name\n",
        "\n",
        "# dummy code referral_source\n",
        "data = pd.get_dummies(data, columns = ['referral_source'])\n",
        "# apply name cleaning again\n",
        "data.columns=data.columns.str.lower().str.replace(' ','_')\n",
        "\n",
        "# convert columns to appropriate data type\n",
        "data=data.convert_dtypes()\n",
        "\n",
        "# replace pandas NAs with numpy NaNs\n",
        "data = data.replace(pd.NA, np.nan)\n",
        "\n",
        "# clean and create target\n",
        "data[['target','class_number']] = data['class'].str.split(\".\", expand=True)\n",
        "data['class_number'] = data['class_number'].str.replace('|','')\n",
        "# code target to 1/0\n",
        "data['target'] = np.where(data['target'] == \"negative\", 0, 1)\n",
        "# convert target to boolean\n",
        "data=data.convert_dtypes()\n",
        "\n",
        "# remove some uneeded columns\n",
        "data=data.drop(['class', 'class_number', 'tbg', 'tbg_measured'], axis = 1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uScGDQqETrkO",
        "outputId": "823cbb44-725c-4a04-9894-a3143bdde902"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:37: FutureWarning: The default value of regex will change from True to False in a future version. In addition, single character regular expressions will *not* be treated as literal strings when regex=True.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.experimental import enable_iterative_imputer\n",
        "from sklearn.impute import IterativeImputer\n",
        "\n",
        "# performing MICE imputation, with hyperparameter values based on assignment 2\n",
        "my_imputer = IterativeImputer(\n",
        "  missing_values=np.nan, # specify missing here to make sure we use np NaNs\n",
        "  random_state=0, # keep random number generator as 0 (there was a typo from assignment 2 that set it as 3)\n",
        "  n_nearest_features=3, # due to analysis from assignment 2\n",
        "  max_iter=2, # changed from 1 to 2 based on the analysis from assignment 2\n",
        "  sample_posterior=True, # need to set to true for this iterative imputer to work\n",
        ")\n",
        "\n",
        "imputed_data = pd.DataFrame(\n",
        "  my_imputer.fit_transform(data), # impute\n",
        "  columns = data.columns # specify column names as it returns numbers\n",
        ")\n",
        "\n",
        "# fixing datatypes\n",
        "imputed_data=imputed_data.convert_dtypes()\n",
        "imputed_data['target'] = imputed_data['target'].astype(bool) # so that kfold strat will work\n",
        "imputed_data.info()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-GZL6bWAT2BC",
        "outputId": "e6c07b5c-4708-4726-e5b9-93c180ee8562"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 3076 entries, 0 to 3075\n",
            "Data columns (total 32 columns):\n",
            " #   Column                     Non-Null Count  Dtype  \n",
            "---  ------                     --------------  -----  \n",
            " 0   age                        3076 non-null   Float64\n",
            " 1   sex_male                   3076 non-null   Float64\n",
            " 2   on_thyroxine               3076 non-null   Int64  \n",
            " 3   query_on_thyroxine         3076 non-null   Int64  \n",
            " 4   on_antithyroid_medication  3076 non-null   Int64  \n",
            " 5   sick                       3076 non-null   Int64  \n",
            " 6   pregnant                   3076 non-null   Int64  \n",
            " 7   thyroid_surgery            3076 non-null   Int64  \n",
            " 8   i131_treatment             3076 non-null   Int64  \n",
            " 9   query_hypothyroid          3076 non-null   Int64  \n",
            " 10  query_hyperthyroid         3076 non-null   Int64  \n",
            " 11  lithium                    3076 non-null   Int64  \n",
            " 12  goitre                     3076 non-null   Int64  \n",
            " 13  tumor                      3076 non-null   Int64  \n",
            " 14  hypopituitary              3076 non-null   Int64  \n",
            " 15  psych                      3076 non-null   Int64  \n",
            " 16  tsh_measured               3076 non-null   Int64  \n",
            " 17  tsh                        3076 non-null   Float64\n",
            " 18  t3_measured                3076 non-null   Int64  \n",
            " 19  t3                         3076 non-null   Float64\n",
            " 20  tt4_measured               3076 non-null   Int64  \n",
            " 21  tt4                        3076 non-null   Float64\n",
            " 22  t4u_measured               3076 non-null   Int64  \n",
            " 23  t4u                        3076 non-null   Float64\n",
            " 24  fti_measured               3076 non-null   Int64  \n",
            " 25  fti                        3076 non-null   Float64\n",
            " 26  referral_source_stmw       3076 non-null   Int64  \n",
            " 27  referral_source_svhc       3076 non-null   Int64  \n",
            " 28  referral_source_svhd       3076 non-null   Int64  \n",
            " 29  referral_source_svi        3076 non-null   Int64  \n",
            " 30  referral_source_other      3076 non-null   Int64  \n",
            " 31  target                     3076 non-null   bool   \n",
            "dtypes: Float64(7), Int64(24), bool(1)\n",
            "memory usage: 841.2 KB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Split data"
      ],
      "metadata": {
        "id": "c1x6oL7bVI2H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "y = imputed_data.loc[:, 'target'] # target\n",
        "x = imputed_data.drop(['target'], axis=1) # features (drop target only)\n",
        "\n",
        "# do an 80/20 split\n",
        "x_train, x_test, y_train, y_test = train_test_split(\n",
        "  x,\n",
        "  y, \n",
        "  train_size=0.8, \n",
        "  test_size=None,\n",
        "  random_state=10, # use random seed 10 for rest of document\n",
        "  shuffle=True,\n",
        "  stratify=y # to retain similar proportions in the samples\n",
        ")"
      ],
      "metadata": {
        "id": "sNEVSJWQVIG1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task 2. Bagging classifiers: Random forests\n",
        "• Train and test a RandomForestClassifier from the ensemble module of scikit-learn (link).\n",
        "\n",
        "• Use RandomizedSearchCV to tune the hyperparameters of the classifier.\n",
        "\n",
        "• Report the performance of the best classifier from the randomized search using the most\n",
        "suitable scoring metric(s) (link) for your dataset."
      ],
      "metadata": {
        "id": "xYgQyBb1ZJV_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "There are several parameters that we don't need to tune for this classifier"
      ],
      "metadata": {
        "id": "sDeSpXk7-LZF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "RandomForestClassifier().get_params()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "crPda1Lc-Es8",
        "outputId": "1851876e-2e59-4877-83a3-d78fac47a398"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'bootstrap': True,\n",
              " 'ccp_alpha': 0.0,\n",
              " 'class_weight': None,\n",
              " 'criterion': 'gini',\n",
              " 'max_depth': None,\n",
              " 'max_features': 'auto',\n",
              " 'max_leaf_nodes': None,\n",
              " 'max_samples': None,\n",
              " 'min_impurity_decrease': 0.0,\n",
              " 'min_samples_leaf': 1,\n",
              " 'min_samples_split': 2,\n",
              " 'min_weight_fraction_leaf': 0.0,\n",
              " 'n_estimators': 100,\n",
              " 'n_jobs': None,\n",
              " 'oob_score': False,\n",
              " 'random_state': None,\n",
              " 'verbose': 0,\n",
              " 'warm_start': False}"
            ]
          },
          "metadata": {},
          "execution_count": 61
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Tuning hyperparameter"
      ],
      "metadata": {
        "id": "-xqgORpW_niu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "\n",
        "# tuning hyperparameters here\n",
        "n_estimators = [int(i) for i in np.linspace(start = 200, stop = 2000, num = 10)]\n",
        "\n",
        "max_features = ['auto', None] # test both automatic and no max features\n",
        "\n",
        "max_depth = [int(i) for i in np.linspace(10, 110, num = 11)] # test max depth in every 10 intervals\n",
        "max_depth.append(None) # also use no max depth\n",
        "\n",
        "min_samples_split = [2, 5, 10] # test 3 different values for min sample\n",
        "\n",
        "min_samples_leaf = [1, 2, 4] \n",
        "\n",
        "bootstrap = [True, False] # try to use both bootstrap and non-bootstrapped samples\n",
        "# if false, whole dataset used to build each tree\n",
        "\n",
        "random_grid = {'n_estimators': n_estimators,\n",
        "               'max_features': max_features,\n",
        "               'max_depth': max_depth,\n",
        "               'min_samples_split': min_samples_split,\n",
        "               'min_samples_leaf': min_samples_leaf,\n",
        "               'bootstrap': bootstrap}\n",
        "\n",
        "# in this example, the random forest is the best classifier of all others\n",
        "rf = RandomForestClassifier(random_state=10) \n",
        "\n",
        "# this only conserves a random subset, more computationally intensive\n",
        "# can use this for finding max interations\n",
        "rf_randsearch = RandomizedSearchCV(estimator=rf, \n",
        "                               param_distributions=random_grid, # give list of different parameters to try \n",
        "                               n_iter=100, # specify n random combinations\n",
        "                               cv=3, # use 3-fold CV, 5 fold might take too long\n",
        "                               verbose=2, \n",
        "                               random_state=10, \n",
        "                               n_jobs=-1)\n",
        "\n",
        "rf_search = rf_randsearch.fit(x_train, y_train)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "byf4YT4Pk4xA",
        "outputId": "310322e1-35f4-4707-aa8f-bd10744d83c2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting 3 folds for each of 100 candidates, totalling 300 fits\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "rf_search.best_params_"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eU3wP8PKrYz9",
        "outputId": "dbf8d9cd-3455-4c73-90a3-ec51e9dd1aec"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'n_estimators': 1600,\n",
              " 'min_samples_split': 10,\n",
              " 'min_samples_leaf': 1,\n",
              " 'max_features': None,\n",
              " 'max_depth': 70,\n",
              " 'bootstrap': False}"
            ]
          },
          "metadata": {},
          "execution_count": 63
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Evaluation metrics\n",
        "\n",
        "The randomized search has provided the best hyperparameters above. Now we will use this to report the performance using several scoring metrics.\n",
        "\n",
        "Our target is very imbalanced, so we should not use accuracy as an evaluation metric for classifiers. There are a couple of useful metrics for imbalanced data including: \n",
        "\n",
        "* AUC\n",
        "* Cohen's Kappa\n",
        "* F1 Score\n",
        "* Jaccard Score\n",
        "\n",
        "So we'll try all of these to see how our classifier does. We'll also include recall"
      ],
      "metadata": {
        "id": "ZAYfOyQNrjH0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import (\n",
        "    cohen_kappa_score,\n",
        "    f1_score,\n",
        "    roc_auc_score,\n",
        "    jaccard_score,\n",
        "    recall_score,\n",
        "    accuracy_score # include accuracy (for fun)\n",
        ")\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "# loop over several metrics for the base model and the optimized model\n",
        "for metric in [cohen_kappa_score, f1_score, roc_auc_score, jaccard_score, recall_score, accuracy_score]:\n",
        "        score_name = metric.__name__\n",
        "        base_model = RandomForestClassifier(random_state=10)\n",
        "        base_model.fit(x_train.values, y_train.values)\n",
        "        \n",
        "        # base model\n",
        "        base_predict = base_model.predict(x_test.values)\n",
        "        print('For base Random Forest:')\n",
        "        base_score = metric(y_test, base_predict)\n",
        "        print('\\t', score_name, ':', base_score.round(4))\n",
        "        #print(classification_report(y_test, base_predict))\n",
        "\n",
        "        #model with best hyperparameters\n",
        "        best_random = rf_search.best_estimator_\n",
        "\n",
        "        random_predict = best_random.predict(x_test.values)\n",
        "        print('For the best combination of hyperparameters of Random Forest:')\n",
        "        random_score = metric(y_test, random_predict)\n",
        "        print('\\t', score_name, ':', random_score.round(4))\n",
        "        #print(classification_report(y_test, random_predict))\n",
        "        \n",
        "        print('Improvement of {:0.2f}%.'.format( 100 * (random_score - base_score) / base_score), \"\\n\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kP_scjBKsnS3",
        "outputId": "52bb3922-e29e-42a6-c661-99e2035e9f0a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "For base Random Forest:\n",
            "\t cohen_kappa_score : 0.952\n",
            "For the best combination of hyperparameters of Random Forest:\n",
            "\t cohen_kappa_score : 0.9708\n",
            "Improvement of 1.97%. \n",
            "\n",
            "\n",
            "For base Random Forest:\n",
            "\t f1_score : 0.9565\n",
            "For the best combination of hyperparameters of Random Forest:\n",
            "\t f1_score : 0.9735\n",
            "Improvement of 1.77%. \n",
            "\n",
            "\n",
            "For base Random Forest:\n",
            "\t roc_auc_score : 0.9723\n",
            "For the best combination of hyperparameters of Random Forest:\n",
            "\t roc_auc_score : 0.9741\n",
            "Improvement of 0.18%. \n",
            "\n",
            "\n",
            "For base Random Forest:\n",
            "\t jaccard_score : 0.9167\n",
            "For the best combination of hyperparameters of Random Forest:\n",
            "\t jaccard_score : 0.9483\n",
            "Improvement of 3.45%. \n",
            "\n",
            "\n",
            "For base Random Forest:\n",
            "\t recall_score : 0.9483\n",
            "For the best combination of hyperparameters of Random Forest:\n",
            "\t recall_score : 0.9483\n",
            "Improvement of 0.00%. \n",
            "\n",
            "\n",
            "For base Random Forest:\n",
            "\t accuracy_score : 0.9919\n",
            "For the best combination of hyperparameters of Random Forest:\n",
            "\t accuracy_score : 0.9951\n",
            "Improvement of 0.33%. \n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Overall, using the tuned hyperparameters improved all evaluation metrics. The metric with the largest improvement was the `jaccard_score`."
      ],
      "metadata": {
        "id": "28SmyJvk7nbM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task 3. Boosting classifiers: Gradient boosting\n",
        "• Train and test a GradientBoostingClassifier from the ensemble module of scikit-learn (link).\n",
        "\n",
        "• Use RandomizedSearchCV to tune the hyperparameters of the classifier.\n",
        "\n",
        "• Report the performance of the best classifier from the randomized search using the most\n",
        "suitable scoring metric(s) (link) for your dataset."
      ],
      "metadata": {
        "id": "LEbv3pjUZY8q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import GradientBoostingClassifier\n",
        "GradientBoostingClassifier().get_params()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OS6HscOF98Z9",
        "outputId": "cf4bd78a-41b4-43b1-d862-bb482409e664"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'ccp_alpha': 0.0,\n",
              " 'criterion': 'friedman_mse',\n",
              " 'init': None,\n",
              " 'learning_rate': 0.1,\n",
              " 'loss': 'deviance',\n",
              " 'max_depth': 3,\n",
              " 'max_features': None,\n",
              " 'max_leaf_nodes': None,\n",
              " 'min_impurity_decrease': 0.0,\n",
              " 'min_samples_leaf': 1,\n",
              " 'min_samples_split': 2,\n",
              " 'min_weight_fraction_leaf': 0.0,\n",
              " 'n_estimators': 100,\n",
              " 'n_iter_no_change': None,\n",
              " 'random_state': None,\n",
              " 'subsample': 1.0,\n",
              " 'tol': 0.0001,\n",
              " 'validation_fraction': 0.1,\n",
              " 'verbose': 0,\n",
              " 'warm_start': False}"
            ]
          },
          "metadata": {},
          "execution_count": 65
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Tuning hyperparameter\n",
        "\n",
        "I will tune the hyperparameters by including values above and below the default values shown above, and also to include values of different magitudes. The most important hyperparameters are `n_estimators` (number of trees) and `max_depth`. This will hopefully yield results that are better than the base model."
      ],
      "metadata": {
        "id": "j1XGI3MA_OwT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "from sklearn.ensemble import GradientBoostingClassifier\n",
        "\n",
        "# tuning hyperparameters here\n",
        "n_estimators = [int(i) for i in np.linspace(start = 100, stop = 1000, num = 11)]\n",
        "\n",
        "max_features = ['auto', None] # test both automatic and no max features\n",
        "\n",
        "max_depth = [int(i) for i in np.linspace(2, 10, num = 5)]\n",
        "max_depth.append(None) # also use no max depth\n",
        "\n",
        "min_samples_split = [2, 5, 10] # test 3 different values for min sample\n",
        "\n",
        "min_samples_leaf = [1, 2, 4] \n",
        "\n",
        "learning_rate = [0.01, 0.025, 0.05, 0.075, 0.1, 0.15, 0.2]\n",
        "\n",
        "random_grid_gb = {'n_estimators': n_estimators,\n",
        "               'max_features': max_features,\n",
        "               'max_depth': max_depth,\n",
        "               'min_samples_split': min_samples_split,\n",
        "               'min_samples_leaf': min_samples_leaf,\n",
        "               'learning_rate': learning_rate}\n",
        "\n",
        "gb = GradientBoostingClassifier(random_state=10) \n",
        "\n",
        "# this only conserves a random subset, we can't search every combination because it would take forever\n",
        "# can use this for finding max interations\n",
        "gb_randsearch = RandomizedSearchCV(estimator=gb, \n",
        "                                  param_distributions=random_grid_gb, # give list of different parameters to try \n",
        "                                  n_iter=100, # specify n random combinations\n",
        "                                  cv=3, # use 3-fold CV, 5 fold might take too long\n",
        "                                  verbose=0, # to make it faster\n",
        "                                  random_state=10, # using a different random seed here because results were worse\n",
        "                                  n_jobs=-1)\n",
        "\n",
        "search_gb = gb_randsearch.fit(x_train, y_train)"
      ],
      "metadata": {
        "id": "MudbCTM4FCJw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "search_gb.best_params_"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Rsrngu8vr8GV",
        "outputId": "55e412bf-52f6-4126-ef23-0918e34f6cfd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'n_estimators': 460,\n",
              " 'min_samples_split': 10,\n",
              " 'min_samples_leaf': 2,\n",
              " 'max_features': None,\n",
              " 'max_depth': 4,\n",
              " 'learning_rate': 0.2}"
            ]
          },
          "metadata": {},
          "execution_count": 67
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Evaluation metrics\n",
        "\n",
        "We will use a similar strategy as in task 2 to report evaluation scores when using the best hyperparameters"
      ],
      "metadata": {
        "id": "61ugTpyLIXQS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# loop over several metrics for the base model and the optimized model\n",
        "for metric in [cohen_kappa_score, f1_score, roc_auc_score, jaccard_score, recall_score, accuracy_score]:\n",
        "        score_name = metric.__name__\n",
        "        base_model = GradientBoostingClassifier(random_state=10)\n",
        "        base_model.fit(x_train.values, y_train.values)\n",
        "        \n",
        "        # base model\n",
        "        base_predict = base_model.predict(x_test.values)\n",
        "        print('For base XGBoost:')\n",
        "        base_score = metric(y_test.values, base_predict)\n",
        "        print('\\t', score_name, ':', base_score.round(4))\n",
        "        #print(classification_report(y_test, base_predict))\n",
        "\n",
        "        #model with best hyperparameters\n",
        "        best_random = search_gb.best_estimator_\n",
        "\n",
        "        random_predict = best_random.predict(x_test.values)\n",
        "        print('For the best combination of hyperparameters of XGBoost:')\n",
        "        random_score = metric(y_test.values, random_predict)\n",
        "        print('\\t', score_name, ':', random_score.round(4))\n",
        "        #print(classification_report(y_test, random_predict))\n",
        "        \n",
        "        print('Improvement of {:0.2f}%.'.format( 100 * (random_score - base_score) / base_score), \"\\n\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XzPlA74FIj7P",
        "outputId": "c9e98fdf-90e4-4149-a057-9eb72acffc64"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "For base XGBoost:\n",
            "\t cohen_kappa_score : 0.9904\n",
            "For the best combination of hyperparameters of XGBoost:\n",
            "\t cohen_kappa_score : 0.9607\n",
            "Improvement of -3.00%. \n",
            "\n",
            "\n",
            "For base XGBoost:\n",
            "\t f1_score : 0.9913\n",
            "For the best combination of hyperparameters of XGBoost:\n",
            "\t f1_score : 0.9643\n",
            "Improvement of -2.73%. \n",
            "\n",
            "\n",
            "For base XGBoost:\n",
            "\t roc_auc_score : 0.9914\n",
            "For the best combination of hyperparameters of XGBoost:\n",
            "\t roc_auc_score : 0.9655\n",
            "Improvement of -2.61%. \n",
            "\n",
            "\n",
            "For base XGBoost:\n",
            "\t jaccard_score : 0.9828\n",
            "For the best combination of hyperparameters of XGBoost:\n",
            "\t jaccard_score : 0.931\n",
            "Improvement of -5.26%. \n",
            "\n",
            "\n",
            "For base XGBoost:\n",
            "\t recall_score : 0.9828\n",
            "For the best combination of hyperparameters of XGBoost:\n",
            "\t recall_score : 0.931\n",
            "Improvement of -5.26%. \n",
            "\n",
            "\n",
            "For base XGBoost:\n",
            "\t accuracy_score : 0.9984\n",
            "For the best combination of hyperparameters of XGBoost:\n",
            "\t accuracy_score : 0.9935\n",
            "Improvement of -0.49%. \n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Tuning the hyperparameters took a really long time, and it doesn't seem that tuning improved the model performance at all. There are several factors that contribute to this:\n",
        "\n",
        "* The base XGBoost performed excellently, making it difficult to improve upon\n",
        "* The randomized search only looked at 100 random combinations, if there were more time and resources we could increase the number of interations.\n",
        "* It may be better to instead perform a normal grid search around the default hyperparameters of XGBoost.\n",
        "\n",
        "XGBoost is known to be a good out-of-the-box algorithm, so this is not surprising."
      ],
      "metadata": {
        "id": "-jhBBAw1bH6x"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task 4. Interpret the results\n",
        "• Using the evaluation metrics applied in tasks 2 and 3, explain which classifier has a better performance by comparing their bias and variance."
      ],
      "metadata": {
        "id": "J7QyDyUsZf9G"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The **bias** is \"How close the model can get to the true relationship between the predictors and the outcome.\"\n",
        "\n",
        "The **variance** \"refers to the amount by which the model would change if we estimated it using a different training data set.'\n",
        "\n",
        "The bias and the variance of a model’s performance are connected. Simple models have high bias, low variance. Simpler models like linear regression have low bias, \n",
        "\n",
        "Underfitting = High bias, High variance\n",
        "\n",
        "Overfitted = Low bias, High variance\n",
        "\n",
        "Balanced model = low bias, low variance\n",
        "\n",
        "Source: https://machinelearningmastery.com/calculate-the-bias-variance-trade-off/"
      ],
      "metadata": {
        "id": "qdGXwNPeThDZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Random Forest "
      ],
      "metadata": {
        "id": "x8yIiWchXkCA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from mlxtend.evaluate import bias_variance_decomp\n",
        "\n",
        "# estimate bias and variance for optimized random forest\n",
        "loss, bias, var = bias_variance_decomp(rf_search.best_estimator_, \n",
        "                                       x_train.values, y_train.values, x_test.values, y_test.values, \n",
        "                                       loss='0-1_loss', # use this instead of mse for binary classification\n",
        "                                       num_rounds=50, # I could use more, but computing power is limited\n",
        "                                       random_seed=10)\n",
        "# summarize results\n",
        "print('Average Expected Loss: %.3f' % loss)\n",
        "print('Average Bias: %.3f' % bias)\n",
        "print('Average Variance: %.3f' % var)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wes4Dk7maaa9",
        "outputId": "aec5e78a-bb2c-4732-c197-ef2419aa5e13"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average Expected Loss: 0.006\n",
            "Average Bias: 0.005\n",
            "Average Variance: 0.003\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## XGBoost with tuned hyperparameters"
      ],
      "metadata": {
        "id": "gphbNqrrXnY0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# estimate bias and variance for optimized XGBoost\n",
        "loss, bias, var = bias_variance_decomp(search_gb.best_estimator_, \n",
        "                                       x_train.values, y_train.values, x_test.values, y_test.values, \n",
        "                                       loss='0-1_loss', # use this instead of mse for binary classification\n",
        "                                       num_rounds=50, # I could use more, but computing power is limited\n",
        "                                       random_seed=10)\n",
        "# summarize results\n",
        "print('Average Expected Loss: %.3f' % loss)\n",
        "print('Average Bias: %.3f' % bias)\n",
        "print('Average Variance: %.3f' % var)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vXy5t3HMp0s7",
        "outputId": "eb966f75-b542-470b-ee74-2916ef5ebc30"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average Expected Loss: 0.006\n",
            "Average Bias: 0.005\n",
            "Average Variance: 0.003\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "From the data above, it is difficult to say that one classifier is better than the other since their bias and variance from `bias_variance_decomp` are basically identical.\n",
        "\n",
        "It is interesting that although both algorithms are tree-based classifiers, the `max_depth` are very different. Random forests have much deeper trees compared to XGBoost. [Source](https://towardsdatascience.com/why-do-random-forest-and-gradient-boosted-decision-trees-have-vastly-different-optimal-max-depth-a64c2f63e127#:~:text=Bias%20measures%20the%20systematic%20loss,patterns%20in%20data%20(overfitting)) \n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "FXfyfUuaXz-S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(search_gb.best_params_)\n",
        "print(rf_search.best_params_)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gjRekB852Sgs",
        "outputId": "698500a0-e398-4460-d265-3862ade8bd22"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'n_estimators': 460, 'min_samples_split': 10, 'min_samples_leaf': 2, 'max_features': None, 'max_depth': 4, 'learning_rate': 0.2}\n",
            "{'n_estimators': 1600, 'min_samples_split': 10, 'min_samples_leaf': 1, 'max_features': None, 'max_depth': 70, 'bootstrap': False}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Base XGBoost"
      ],
      "metadata": {
        "id": "S4hAntq01fWg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# estimate bias and variance for optimized XGBoost\n",
        "loss, bias, var = bias_variance_decomp(GradientBoostingClassifier(random_state=10), \n",
        "                                       x_train.values, y_train.values, x_test.values, y_test.values, \n",
        "                                       loss='0-1_loss', # use this instead of mse for binary classification\n",
        "                                       num_rounds=50, # I could use more, but computing power is limited\n",
        "                                       random_seed=10)\n",
        "# summarize results\n",
        "print('Average Expected Loss: %.3f' % loss)\n",
        "print('Average Bias: %.3f' % bias)\n",
        "print('Average Variance: %.3f' % var)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U9j5Q8zm1bqE",
        "outputId": "979f24c5-5d9a-442c-9075-827b33ed4fc3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average Expected Loss: 0.006\n",
            "Average Bias: 0.005\n",
            "Average Variance: 0.004\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The default XGBoost also has almost identical average bias and variance over 50 bootstrapped samples as the previous. Here, I'll conclude that the default XGBoost is the best performing model since it has the highest scoring in the evaluation metrics: \n",
        "    \n",
        "    cohen_kappa_score\n",
        "    f1_score\n",
        "    roc_auc_score\n",
        "    jaccard_score\n",
        "    recall_score\n",
        "\n",
        "all the while having similar bias and variance."
      ],
      "metadata": {
        "id": "Lt7t6f-k3n6A"
      }
    }
  ]
}